<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HongYan's homepage</title>
    <description>I am an phD student of Computer Science and Technology in Shanghai Jiao Tong University (SJTU). I major in computer vision and deep learning. Currently, I focus on meta-learning for few-shot image generation. Prior to now, I was a master at SJTU. Before that, I obtained my bachelor degree from Shenzhen University(SZU).</description>
    <link>https://www.aiforall.pro</link>
    <atom:link href="https://www.aiforall.pro/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>总结回顾：2018年，云深不知处</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;只在此山中， 云深不知处。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从16年开始工作至今，每过一年，都有一种错觉，好像自己变得更加成熟了，比如工作技能、做事方法的提升，但刨去这一切跟&lt;strong&gt;知识或者说技能&lt;/strong&gt;相关的，又发觉自己获取到的&lt;strong&gt;智慧&lt;/strong&gt;其实并没有进入快速上升的通道，这大概就是所说的”技能容易获取，智慧不易修炼”吧。&lt;/p&gt;

&lt;p&gt;每年都写年终（中）总结，完结的，未完结的（比如17年年终总结因为不可预知的懒癌原因，现在还一直处于半成品状态&lt;a href=&quot;https://github.com/willard-yuan/willard-yuan.github.io/blob/master/_draft/2018-02-26-year-turned-back.md&quot;&gt;2017年，成长无限大&lt;/a&gt;），在流水式的记录里，通过不断的总结、反思，期待自己来年会比过去的一年，做得更好，活得更出彩。&lt;/p&gt;

&lt;h2 id=&quot;工作&quot;&gt;工作&lt;/h2&gt;

&lt;p&gt;18年、17年两年时间，在KS公司围绕以视觉检索为核心技术，做了很多计算机视觉相关的业务。回顾这两年，无论对专业领域的理解深度，以及对相关领域的了解广度，个人都取得了一些较快的进步、较好的成绩以及满意的回报，这其中个人的努力固不可少，但也非常得益于K公司业务的高速发展和很赏识认可自己的老大。&lt;/p&gt;

&lt;p&gt;一年时间，一个人可以做很多的工作，重要的、琐碎的、维护性质的，就好像一盘从海里打捞起来的沙，那些重要的工作，就是珍珠，不必多，三两颗足以，但要非常亮眼。这里的亮眼，可以从两个维度来衡量：一是对线上起到非常大的正向影响，二是虽没有正式产生影响，但leader非常看重，技术有非常大的突破。回头看过去两年，自己写的工作总结，正在顺着这样一种方式走，虽有时候人算不如天算，比如年初计划的全年工作，有可能会被临时来的项目给打乱，但我觉得，以这样一种方式来指导全年的工作结果，不断的修正全年的计划，使得最终年末的时候，达到这样一种工作结果，总是最优的。&lt;/p&gt;

&lt;p&gt;18年，开始指导新入职的应届生同事，并逐步学习怎么拆分、安排任务，为指导的同事提供靠谱的算法研发方向，不用再像17年那样，一个人要扛整个视觉检索等相关业务的算法研发。虽然带人需要很多的耐心，但我觉得带得还算称职，我也有机会通过同事他们工作过程中出现的问题，反思自己做事的方式、考虑问题的角度等方面是否也存在这样的问题。&lt;/p&gt;

&lt;h2 id=&quot;技能&quot;&gt;技能&lt;/h2&gt;

&lt;p&gt;技能方面，过去一年成长很多，比如做事方式更有条理，对结果更加负责，怎样有始有终坚持推动一个项目从算法研发、评估、上线、跟踪全流程，新的一年也有很多需要不断改进之处，比如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;个人技能方面，夯实深度之外，应纵向拓展技术的广度；&lt;/li&gt;
  &lt;li&gt;应该更加以结果导向，考虑问题应更加要有整体的概念，比如为什么要做这件事，做这件事的目的是什么，做这件事有哪些方式，可能会出现什么问题，最终的收益是什么；&lt;/li&gt;
  &lt;li&gt;在向上汇报的时候，叙述事情需要更加凝练简洁，先说结果，再讲过程；&lt;/li&gt;
  &lt;li&gt;在带人方面，工作上需严格要求他们，这一点觉得自己做得比较差，主要是特别不喜欢勉强人，喜欢假定人是高度自我驱动的，坚信”师傅领进门，修行靠个人”；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;18年，个人专业技能方面，结合线上出现的各种badcase，深挖视觉检索技术的深度，将度量学习特征表达学习、图ANN索引等最新的工作应用于实际项目中，同时拓展推荐系统领域的知识并参与其中，深度学习框架也全面切入至Tensorflow。知识共享方面，18年博文产出只有4篇，数量是偏少（正常博文数量应该是12篇，一个月一篇博文），这其中次要原因是工作忙、需要时间陪女朋友，主要原因我觉得还是个人自律能力做得不够好，其实一个月拿出一天写一篇高质量的博文，时间上是完全可以支配的；代码开源方面，这一年更注重开源的质量，减少无效的提交，同时因为工作的原因，所以整个一年看下来，在&lt;a href=&quot;https://github.com/willard-yuan&quot;&gt;github&lt;/a&gt;变绿的网格少了很多，但项目被star和follow自己的人数，今年反而变得更多了，在此就一一谢谢了。&lt;/p&gt;

&lt;p&gt;总的来说，18年，包括17年，在KS的这两年多时间，无论是专业知识方面，还是工作方式与方法、考虑问题的角度，都有比较快速的进步，对专业领域的知识，经过线上业务的打磨，也有了更深层次的理解。&lt;/p&gt;

&lt;h2 id=&quot;认知&quot;&gt;认知&lt;/h2&gt;

&lt;p&gt;我们常听人称赞某人很有学识，却很少听到被称赞为有智慧。这是由于知识、技能等可以通过密集地学习，相对比较快速的获取，而一个人的格局（比如认知、智慧等），往往需要历经长时的沉思、反省才能渐渐得以提升。回首18年，抛却知识、技能等非常功利性的方面，自己在认知、智慧等安身立命方面的成长其实是非常小的。这其中原因，除了智慧本身不易获取外，个人方面主要还是意志力随岁月而逐渐衰减，此外，也会受到一些次要客观因素的干扰，比如工作太忙、放假了想休息、自身不再是一个独立的个体等。&lt;/p&gt;

&lt;p&gt;心态上，18年相比17年，越来越从内心接受这种日复一日工作的日子，就好似17年内心那匹脱缰的马儿，不仅栓着了，而且还被遗忘了。这样的一种心态，对自己而言，或许是目前最好的，不困于心，安然自乐，并且比较清晰地知道工作的意义和奋斗的目标。&lt;/p&gt;

&lt;p&gt;18年，读书方面，除了阅读《乌合之众》《人类简史》《重来：更为简单有效的思维方式》《创业维艰》《国富论》《纳什均衡与博弈论》《中国哲学简史》《明清海盗的兴衰》《曾国藩家书》之外，也开始比较完整的读一些投资理论方面的书籍，比如《财务自由之路》《投资中不简单的事》《聪明的投资者》，特别赞同段永平先生说过的一句话：越接近人类经济活动的地方，往往越接近事物的认知本质。总的来说，18年其实是有时间可以读更多经典的书籍的，这一点做得不太好。在19年，希望能读更多经典的书籍，对于一些读完收获颇大的书，更应该做些笔记最好。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E9%9D%92%E5%8E%9F%E8%A1%8C%E6%80%9D&quot;&gt;青原行思&lt;/a&gt;大师有云：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;看山是山，看山不是山，看山还是山&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所有一切认知上的折腾，都不过是以求悟道后“看山还是山”。不执于相，不囿于空，不持戒却不坏法性，随心所欲而不愈矩，on the way。&lt;/p&gt;

&lt;h2 id=&quot;生活&quot;&gt;生活&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;我想宁静地走自己的路，不被打扰。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对生活最美好的追求，是牵着亲爱的手，一步步宁静地，追寻更好的生活与未来。&lt;/p&gt;

&lt;p&gt;2018年，在工作、生活袅绕的云雾里，全情置身认知的深山，不知身在何处，但我知道，无限风景在西峰。&lt;/p&gt;
</description>
        <pubDate>Sun, 07 Apr 2019 00:00:00 +0800</pubDate>
        <link>https://www.aiforall.pro//blog/2019/04/2018-year-end-summary/</link>
        <guid isPermaLink="true">https://www.aiforall.pro//blog/2019/04/2018-year-end-summary/</guid>
      </item>
    
      <item>
        <title>机器视觉：图像与视频朝向检测</title>
        <description>&lt;p&gt;在图片社交、短视频等行业，用户在导入图片、视频的时候，偶尔会导入一些横屏拍摄的视频，使得图片、视频在呈现给用户观看的时候，是旋转了90度或者270度的图片、视频（180度一般极其少见），从而给用户造成不好的体验，特别是以瀑布流方式展示的app。这类视频，由于导入的时候，拿不到原始的拍摄标识信息，从而无法直接取到图像、视频是否旋转的信息，而必须依赖视觉识别的方式，去判断图像、视频是否旋转。&lt;/p&gt;

&lt;p&gt;对图像朝向判断问题，学术上研究得比较少，已有的论文主要还是以深度学习分类的方式去检测，比如：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.toronto.edu/~guerzhoy/oriviz/crv17.pdf&quot;&gt;Automatic Photo Orientation Detection with Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1712.01195.pdf&quot;&gt;Why My Photos Look Sideways or Upside Down? Detecting Canonical Orientation of Images Using Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对图像Orientation Detection的难点主要在于：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当图片、视频中的物体有倾斜，但是图片、视频本身是正常放置的，容易造成误检。比如拍摄是一张大脸，但大脸有倾斜，或者拍摄的是一个床上斜躺着的宝宝；&lt;/li&gt;
  &lt;li&gt;当拍摄角度是向下或者向上的时，这类视频容易造成误分；比如拍摄天空、拍摄天花板等；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以图像Orientation Detection，在保证召回较好的前提下，准确率要做得非常好（准确率99%+以上，便于机器自动处理）还是很不容易的，特别是针对单张图片检测更是难上加难。不过，如果是短视频行业，由于短视频行业自身的一些特性，使得这个问题还是可以比较好的解决的。在短视频行业，做图像Orientation Detection可以利用的优势主要有三点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数据优势。训练数据不用标注，直接使用app自带的相机拍摄的视频，这些视频可以认为是0度放置的，通过此种方式，可以获取到无穷无尽的标签噪声非常少的训练数据；&lt;/li&gt;
  &lt;li&gt;多帧检测优势。利用多帧检测的方式，可以极大地提升检测的准确率；&lt;/li&gt;
  &lt;li&gt;Orientation Detection类型比较固定。通常主要是0度、90度和270度，180度几乎不会出现，也就是用户导入视频进来的时候，如果有旋转，主要是横屏，几乎不会存在把视频上下反转一下的情况；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;有了0度的训练数据，其他角度的数据，比如90度、270度（180度可以丢弃不检测，因为出现这种导入情况的极少）可以通过0度变换一下便可得到。有了0度、90度、270度的训练数据，训练可以采用CNN分类模型直接分类即可。&lt;/p&gt;

&lt;h3 id=&quot;检测准确率提升&quot;&gt;检测准确率提升&lt;/h3&gt;

&lt;p&gt;分类模型训练好了后，如果只检测一帧，准确率往往是不够的，比如上面列举的两种情况，就极易以非常高的概率导致误分，为了提升检测的准确率，并保持还不错的召回率，可以通过联合使用下面两种方式使得最终的检测准确率达到99%+，这两种方式分别为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;检测多帧。比如一个视频检测3帧；&lt;/li&gt;
  &lt;li&gt;设置的检测概率相对高一点。比如检测3帧，两帧朝向一致，且两帧最小概率大于0.9的，则判断为旋转，旋转方向为两帧朝向的方向；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过这样两种方式，可以在保持召回还不错的情况下，获得较高的准确率，最终达到机器自动处理的目的。&lt;/p&gt;

&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;

&lt;p&gt;视频朝向检测有自身多帧的优势，如果是图片社交app，单张图片要达到非常高的准确率是极其困难的（在保持一定召回情况下）。&lt;/p&gt;
</description>
        <pubDate>Sun, 06 Jan 2019 00:00:00 +0800</pubDate>
        <link>https://www.aiforall.pro//blog/2019/01/image-orientation-detection/</link>
        <guid isPermaLink="true">https://www.aiforall.pro//blog/2019/01/image-orientation-detection/</guid>
      </item>
    
      <item>
        <title>图像检索：INS视觉检索</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;INS is a image retrieval system for instance search. The system can be used to retrieve same object, near-duplicate object, and copy detection, and developing the system is just for interest in my free time. I’ll improve the system contiously.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;十一时期，窝在住的地方哪里也没有去，把此前闲着无事开发的INS检索系统优化了一版。优化后的效果如下：&lt;/p&gt;

&lt;section&gt;
    &lt;video id=&quot;my-video&quot; class=&quot;video-js&quot; controls=&quot;&quot; preload=&quot;auto&quot; width=&quot;760&quot; height=&quot;475&quot; poster=&quot;http://yongyuan.name/project/imgs/ins.jpg&quot; data-setup=&quot;{}&quot;&gt;
    &lt;source src=&quot;http://yongyuan.name/project/videos/ins.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    &lt;p class=&quot;vjs-no-js&quot;&gt;
      To view this video please enable JavaScript, and consider upgrading to a web browser that
      &lt;a href=&quot;http://videojs.com/html5-video-support/&quot; target=&quot;_blank&quot;&gt;supports HTML5 video&lt;/a&gt;
    &lt;/p&gt;
    &lt;/video&gt;
&lt;/section&gt;

&lt;h2 id=&quot;ins概览&quot;&gt;INS概览&lt;/h2&gt;

&lt;p&gt;系统开发采用的C++和QT，在开发成桌面版本之前，复用&lt;a href=&quot;https://github.com/zysite/SoTu&quot;&gt;SoTu&lt;/a&gt;的web界面开发了一版web系统（开发的web演示系统，将会放在&lt;a href=&quot;https://github.com/willard-yuan/flask-keras-cnn-image-retrieval&quot;&gt;flask-keras-cnn-image-retrieval&lt;/a&gt;中），发觉使用起来不是很方面，然后切换成桌面开发。整个开发流程相对都比较流畅，主要得益于下面两点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;QT与C++无缝衔接。无论是语法还是信号槽等，理解起来都非常地直接，顺畅。&lt;/li&gt;
  &lt;li&gt;OpenCV DNN模块带来的便捷。OpenCV从3.3开始，加入了DNN模块，所以不论是TensorFlow还是Caffe等主流框架训练的模型，都可以使用OpenCV做推理，不用引入一堆依赖库。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整个自由开发项目，小白菜收获了主要有3点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;视觉检索特征学习方式进一步深入与拓宽。这一点在后面展开。&lt;/li&gt;
  &lt;li&gt;QT多窗口通信方式。比如主窗口打开设置参数的窗口，将参数窗口设置的信息传递给主窗口。&lt;/li&gt;
  &lt;li&gt;完整地打包软件。将整个项目打包，发布成一个完整的桌面安装包。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整个系统的开发，对小白菜的工程能力提升最大，因为在开发整个系统之初，整个模型差不多已经训练完了，剩下的就是把它搭成一个完整的桌面应用。下面针对视觉检索系统的特征与索引聊聊最近新收获的感悟。&lt;/p&gt;

&lt;h2 id=&quot;ins特征学习&quot;&gt;INS特征学习&lt;/h2&gt;

&lt;p&gt;INS特征学习有其相对比较固定的思路，但是在不同的应用场景下，其特征学习的方式应该做相应的调整。简单的列举3个常见的场景：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;拷贝检测场景。最常见的工业应用场景有短视频、图片拷贝检测（查重），实现流量分发控制，从而到达数字版权保护的目的。&lt;/li&gt;
  &lt;li&gt;实例检索场景。比如电商实例检索，比如拍立淘、Pinterst、&lt;a href=&quot;Introducing Visual Search&quot;&gt;Snapchat&lt;/a&gt;等主流的电商、社交平台都有自己的视觉搜索系统。&lt;/li&gt;
  &lt;li&gt;相似检索场景。图片推荐系统比如图片广告推荐、相似视频推荐等场景皆有应用，人脸检索里面也可以用相似人脸做一些比较有趣的创意。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相似是一个比较主观的东西，相同可以看成是相似的特例，按照这种范畴划分，在做视觉拷贝检测的时候，我们采用相似检索方式训练模型是没有多大问题的，但是将采用这种方式训练出来的模型应用到视频、图片查重中，取得的效果能不能达到最优，显然是一个大大的问号。事实上，经过我们在大规模数据集上实验验证，这种针对相似检索场景训练出来的特征，并没有针对在拷贝场景训练回来的特征更适合视频、图片查重。这里面的核心问题在于：在特征度量空间，由于重复图片形变过大，导致基于相似检索场景训练出来的模型，使得相似的图片更靠近原始图片，而形变过大的重复图片，无论我们的模型如何加约束条件，都不能将它拉得比相似的更近。&lt;/p&gt;

&lt;p&gt;所以，如果是拷贝检测场景，应该单独训练一种能够容忍图像不同形变的特征。实际上，对于拷贝检测场景，特征与图像内容是无关的，也就是说，对于原始图像以及形变后的图像，&lt;strong&gt;特征不需要关注图像里面有什么样的内容&lt;/strong&gt;，只需要关注原始图像的特征与形变后的图像特征，在度量空间里，它们是不是足够的近便可。因而对于拷贝检测这个场景，根据”特征不需要关注图像里面有什么样的内容”这个结论，我们可以得到一条非常实用的数据经验，即我们&lt;strong&gt;不需要对训练数据做分布上的要求&lt;/strong&gt;。比如，为了在A平台上做拷贝检测，我们完全可以使用B平台的数据集（公开数据集），只要在训练的时候，类标签足够的大，图片的形变足够多，那么在B上训练得到的模型，直接在A上应用，也能取得非常好的效果。&lt;/p&gt;

&lt;p&gt;下图是基于相似检索场景和基于拷贝场景训练出来的模型，在100万图库上召回率对比：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/dupnet_simnet.jpg&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图蓝色的曲线和红色的曲线都是基于相似检索场景训练出来的模型，绿色的线是基于拷贝检测场景训练出来的模型，可以看到，采用基于拷贝检测场景训练出来的模型，在视频拷贝检测上，召回来有了非常好的提升。同时，基于这种场景训练出来的模型，在排序上，靠前的跟偏重图片各种物理属性视觉上的相似，而不是语义上的相似。&lt;/p&gt;

&lt;p&gt;前面我们说过，相同可以看成是相似的特例，但是上面的实验结果表明，相似检索场景与拷贝检测场景这两个东西是相悖的，分别处于事物的两个极端。要（语义）相似的话，特征形变能力会减弱；要特征抗形变能力增强，特征（语义）相似性会减弱。这两个似乎无法调和，无法统一到一起。但是，“相同可以看成是相似的特例”确实事实，所以两者虽处于事物的两个极端，但肯定是可以统一到一个框架中的。想想lambda、(1-lambda)以及多任务学习，具体怎么统一到同一个框架中，以后有机会再讲。&lt;/p&gt;

&lt;p&gt;至于实例检索场景，通常需跟物体检测结合起来才能学习好特征，这也间接地说明，面向万物的通用实例检索系统，要做得非常的好，会碰到巨大的障碍。这也是视觉检索为什么会向垂直领域比如人脸检索、电商商品检索等发展的原因。&lt;/p&gt;

&lt;h2 id=&quot;ins索引&quot;&gt;INS索引&lt;/h2&gt;

&lt;p&gt;INS针对的是一款桌面视觉检索软件，所以图库大小适用于中小规模（百万量级），对于中小规模应用场景，为了保证检索的高召回率，INS采用了图ANN索引算法，到具体的方法则采用了层次可导小世界索引算法（HNSW，在之前的博文&lt;a href=&quot;http://yongyuan.name/blog/opq-and-hnsw.html&quot;&gt;OPQ索引与HNSW索引&lt;/a&gt;有相应介绍）。如果要拓展到百亿以上规模的量级，索引可以采用图索引跟矢量量化结合的方法，比如HNSW和OPQ一起结合起来使用。&lt;/p&gt;

&lt;h2 id=&quot;ins后续成长点&quot;&gt;INS后续成长点&lt;/h2&gt;

&lt;p&gt;每个做算法的同学，都踹怀着发明一种非常work、非常solid、非常先进并能成为经典算法的梦想。小白菜也希望以INS为契机，进一步不断在此领域不断深耕细作，最后成为此领域的专家。具体到INS这款自由开发的视觉检索软件，小白菜会在周末自由时间，从下面方向完善INS：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;优化特征抽取模型，最终是把检测模块也加上；&lt;/li&gt;
  &lt;li&gt;索引模块优化，目前索引模块还不是很合理，索引删除未支持；&lt;/li&gt;
  &lt;li&gt;完善INS系统的稳健性，尽量避免出现软件崩溃；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这是小白菜开发的第一款真正可以称得上软件的系统，小白菜期待自己能够坚持下去。另外，INS暂时没有开源的计划，主要考虑到整个系统还不是很完善，效果也不是很好，等时机到了再说。&lt;/p&gt;
</description>
        <pubDate>Sun, 04 Nov 2018 00:00:00 +0800</pubDate>
        <link>https://www.aiforall.pro//blog/2018/11/ins-system/</link>
        <guid isPermaLink="true">https://www.aiforall.pro//blog/2018/11/ins-system/</guid>
      </item>
    
      <item>
        <title>图像检索：OPQ索引与HNSW索引</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;人的独立性和参与性必须适得其所，平衡发展。一方面，过分的参与必然导致远离自我核心，现代人之所以感到空虚、无聊，在很大程度上就是由于顺从、依赖和参与过多，脱离了自我核心。另一方面，过分的独立会将自己束缚在狭小的自我世界内，缺乏正常的交往，必然损害人的正常发展。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;关于索引结构，有千千万万，而在图像检索领域，索引主要是为特征索引而设计的一种数据结构。关于ANN搜索领域的学术研究，&lt;a href=&quot;http://www.itu.dk/people/pagh/&quot;&gt;Rasmus Pagh&lt;/a&gt;发起的大规模相似搜索项目&lt;a href=&quot;http://sss.projects.itu.dk/ann-benchmarks/&quot;&gt;ANN-Benchmarks&lt;/a&gt;、&lt;a href=&quot;https://github.com/facebookresearch/faiss&quot;&gt;Faiss&lt;/a&gt;以及&lt;a href=&quot;https://github.com/erikbern/ann-benchmarks&quot;&gt;ann-benchmarks&lt;/a&gt;都有对一些主流的方法做过对比。虽然三个对比的框架对不同方法的性能均有出入，但一些主流方法的性能差异是可以达成共识的，比如基于图方法的ANN其召回率均要优于其他方法。在工业上，常用的索引方法主要以倒排、&lt;a href=&quot;http://yongyuan.name/blog/ann-search.html&quot;&gt;PQ及其变种&lt;/a&gt;、基于树的方法（比如KD树）和&lt;a href=&quot;https://github.com/willard-yuan/hashing-baseline-for-image-retrieval&quot;&gt;哈希&lt;/a&gt;（典型代表LSH和&lt;a href=&quot;http://yongyuan.name/blog/itq-hashing.html&quot;&gt;ITQ&lt;/a&gt;）为主流。关于KD树、LSH以及PQ，小白菜曾在此前的博文&lt;a href=&quot;http://yongyuan.name/blog/ann-search.html&quot;&gt;图像检索：再叙ANN Search&lt;/a&gt;已有比较详细的介绍。本文是小白菜结合实际应用，对PQ的改进方法OPQ以及基于图的方法HNSW的理解，以及关于索引的一些总结与思考。&lt;/p&gt;

&lt;h2 id=&quot;opq-vs-hnsw&quot;&gt;OPQ vs. HNSW&lt;/h2&gt;

&lt;p&gt;首先从检索的召回率上来评估，基于图的索引方法要优于目前其他一些主流ANN搜索方法，比如乘积量化方法（PQ、OPQ）、哈希方法等。虽然乘积量化方法的召回率不如HNSW，但由于乘积量化方法具备内存耗用更小、数据动态增删更灵活等特性，使得在工业检索系统中，在对召回率要求不是特别高的场景下，乘积量化方法仍然是使用得较多的一种索引方法，淘宝（详见&lt;a href=&quot;https://arxiv.org/abs/1707.00143&quot;&gt;Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph&lt;/a&gt;）、蘑菇街等公司均有使用。乘积量化和HNSW特性对比如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;特性&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;OPQ&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;HNSW&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;内存占用&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;小&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;大&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;召回率&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;较高&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;高&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;数据动态增删&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;灵活&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;不易&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;基于图ANN方法由于数据在插入索引的时候，需要计算（部分）数据间的近邻关系，因而需要实时获取到到数据的原始特征，几乎所有基于图ANN的方法在处理该问题的时候，都是直接将原始特征加载在内存（索引）里，从而造成对内存使用过大，至于召回率图ANN方法要比基于量化的方法要高，这个理解起来比较直观。下面分别对改进的乘积量化方法OPQ以及基于图ANN方法HNSW做原理上的简要介绍。&lt;/p&gt;

&lt;h2 id=&quot;opq&quot;&gt;OPQ&lt;/h2&gt;

&lt;p&gt;OPQ是PQ的一种改进方法，关于PQ的介绍，在此前的文章&lt;a href=&quot;http://yongyuan.name/blog/ann-search.html&quot;&gt;图像检索：再叙ANN Search&lt;/a&gt;中已有详细介绍，这里仅对改进的部分做相应的介绍。&lt;/p&gt;

&lt;p&gt;通常，用于检索的原始特征维度较高，所以实际在使用PQ等方法构建索引的时候，常会对高维的特征使用PCA等降维方法对特征先做降维处理，这样降维预处理，可以达到两个目的：一是降低特征维度；二是在对向量进行子段切分的时候要求特征各个维度是不相关的，做完PCA之后，可以一定程度缓解这个问题。但是这么做了后，在切分子段的时候，采用顺序切分子段仍然存在一定的问题，这个问题可以借用&lt;a href=&quot;http://yongyuan.name/blog/itq-hashing.html&quot;&gt;ITQ&lt;/a&gt;中的一个二维平面的例子加以说明：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/itq_hashing.png&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上面左图（a图）所示，对于PCA降维后的二维空间，假设在做PQ的时候，将子段数目设置为2段，即切分成$x$和$y$两个子向量，然后分别在$x$和$y$上做聚类（假设聚类中心设置为2）。对左图（a图）和右图（c图）聚类的结果进行比较，可以明显的发现，左图在y方向上聚类的效果明显差于右图，而PQ又是采用聚类中心来近似原始向量（这里指降维后的向量），也就是右图是我们需要的结果。这个问题可以转化为数据方差来描述：&lt;strong&gt;在做PQ编码时，对于切分的各个子空间，我们应尽可能使得各个子空间的方差比较接近，最理想的情况是各个子空间的方差都相等&lt;/strong&gt;。上图左图中，$x$和$y$各个方向的方差明显是差得比较大的，而对于右图，$x$和$y$方向各个方向的方差差不多是比较接近的。&lt;/p&gt;

&lt;p&gt;为了在切分子段的时候，使得各个子空间的方差尽可能的一致，&lt;a href=&quot;https://research.fb.com/people/jegou-herve/&quot;&gt;Herve Jegou&lt;/a&gt;在&lt;a href=&quot;https://lear.inrialpes.fr/pubs/2010/JDSP10/jegou_compactimagerepresentation.pdf&quot;&gt;Aggregating local descriptors into a compact image representation&lt;/a&gt;中提出使用一个正交矩阵来对PCA降维后的数据再做一次变换，使得各个子空间的方差尽可能的一致。其对应的待优化目标函数见论文的第5页，由于优化该目标函数极其困难，Herve Jegou使用了Householder矩阵来得到该正交矩阵，但是得到的该正交矩阵并不能很好的均衡子空间的方差。&lt;/p&gt;

&lt;p&gt;OPQ致力于解决的问题正是对各个子空间方差的均衡。具体到方法上，OPQ借鉴了ITQ的思想，在聚类的时候对聚类中心寻找对应的最优旋转矩阵，使得所有子空间中各个数据点到对应子空间的类中心的L2损失的求和最小。OPQ在具体求解的时候，分为非参求解方法和带参求解方法，具体为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;非参求解方法。跟ITQ的求解过程一样。&lt;/li&gt;
  &lt;li&gt;带参求解方法。带参求解方法假设数据服从高斯分布，在此条件下，最终可以将求解过程简化为数据经过PCA分解后，特征值如何分组的问题。在实际中，该解法更具备高实用性。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hnsw&quot;&gt;HNSW&lt;/h2&gt;

&lt;p&gt;HNSW是Yury A. Malkov提出的一种基于图索引的方法，它是Yury A. Malkov在他本人之前工作NSW上一种改进，通过采用层状结构，将边按特征半径进行分层，使每个顶点在所有层中平均度数变为常数，从而将NSW的计算复杂度由多重对数(Polylogarithmic)复杂度降到了对数(logarithmic)复杂度。&lt;/p&gt;

&lt;h3 id=&quot;贡献&quot;&gt;贡献&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;图输入节点明确的选择&lt;/li&gt;
  &lt;li&gt;使用不同尺度划分链接&lt;/li&gt;
  &lt;li&gt;使用启发式方式来选择最近邻&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;近邻图技术&quot;&gt;近邻图技术&lt;/h3&gt;

&lt;p&gt;对于给定的近邻图，在开始搜索的时候，从若干输入点（随机选取或分割算法）开始迭代遍历整个近邻图。&lt;/p&gt;

&lt;p&gt;在每一次横向迭代的时候，算法会检查链接或当前base节点之间的距离，然后选择下一个base节点作为相邻节点，使得能最好的最小化连接间的距离。&lt;/p&gt;

&lt;p&gt;近邻图主要的缺陷：1. 在路由阶段，如果随机从一个或者固定的阶段开始，迭代的步数会随着库的大小增长呈现幂次增加；2. 当使用k-NN图的时候，一个全局连接可能的损失会导致很差的搜索结果。&lt;/p&gt;

&lt;h3 id=&quot;算法描述&quot;&gt;算法描述&lt;/h3&gt;

&lt;p&gt;网络图以连续插入的方式构建。对于每一个要插入的元素，采用指数衰变概率分布函数来随机选取整数最大层。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/hnsw.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;图构建元素插入过程（Algorithm 1）：从顶层开始贪心遍历graph，以便在某层A中找到最近邻。当在A层找到局部最小值之后，再将A层中找到的最近邻作为输入点（entry point），继续在下一层中寻找最近邻，重复该过程；&lt;/li&gt;
  &lt;li&gt;层内最近邻查找（Algorithm 2）：贪心搜索的改进版本；&lt;/li&gt;
  &lt;li&gt;在搜索阶段，维护一个动态列表，用于保持ef个找到的最近邻元素&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在搜索的初步阶段，ef参数设置为1。搜索过程包括zoom out和zoom in两个阶段，zoom out是远程路由，zoom in顾名思义就是在定位的区域做精细的搜索过程。整个过程可以类比在地图上寻找某个位置的过程：我们可以地球当做最顶层，五大洲作为第二层，国家作为第三层，省份作为第四层……，现在如果要找海淀五道口，我们可以通过顶层以逐步递减的特性半径对其进行路由（第一层地球-&amp;gt;第二层亚洲—&amp;gt;第三层中国-&amp;gt;第四层北京-&amp;gt;海淀区），到了第0层后，再在局部区域做更精细的搜索。&lt;/p&gt;

&lt;h3 id=&quot;数据实验说明&quot;&gt;数据实验说明&lt;/h3&gt;

&lt;p&gt;199485332条人脸数据（128维，L2归一化）作为database, 10000条人脸数据作为查询。gound truth由暴力搜索结果产生（余弦相似度），将暴力搜索结果的rank@1作为gound truth，评估top@K下的召回率。&lt;/p&gt;

&lt;h3 id=&quot;实验结果与调优&quot;&gt;实验结果与调优&lt;/h3&gt;

&lt;p&gt;M参数：80，内存大小: 159364 Mb，索引文件：&lt;code class=&quot;highlighter-rouge&quot;&gt;cnn2b_199485332m_ef_80_M_32_ip.bin&lt;/code&gt;，查询样本数目: 10000，ef: 1000，距离：内积距离&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;top@K&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;召回&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;时间(time(us) per query)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.957000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.977300&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9754.885742us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.981200&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9619.380859us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.983100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9652.819336us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.983800&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9628.488281us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.984500&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9650.678711us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;50&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.986400&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9647.286133us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.986700&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9665.638672us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;300&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.987000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9685.414062us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;500&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.987100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9744.437500us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.987100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9804.702148us&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/hnsw_face_2b.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;M参数：16，Mem: 173442 Mb， 索引文件：&lt;code class=&quot;highlighter-rouge&quot;&gt;cnn2b_199485332m_ef_40_M_16.bin&lt;/code&gt;, 查询样本数目: 10000，ef: 1000，距离：欧氏距离&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;top@K&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;召回&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;时间(time_us_per_query)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.887800&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4845.700684us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.911700&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6732.230957us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.916600&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6879.585449us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.917500&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6963.914062us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.918000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6920.318359us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.920200&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6880.795898us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;50&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.922400&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6900.778809us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.923000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6970.664062us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;300&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.923400&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6978.517578us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;500&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.923400&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6992.306152us&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;M参数：40，Mem: 211533 Mb， 索引文件：&lt;code class=&quot;highlighter-rouge&quot;&gt;cnn2b_199485332m_ef_40_M_40.bin&lt;/code&gt;, 查询样本数目: 10000，ef: 1000，距离：内积距离&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;top@K&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;召回&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;时间(time_us_per_query)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.928600&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6448.714355us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.948300&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7658.459961us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.952600&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7674.244629us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.954000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7659.506348us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.954700&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7679.874023us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.955800&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7709.812500us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;50&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.957400&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7720.283691us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.957800&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7722.512695us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;300&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.958000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7763.615234us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;500&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.958100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7779.351562us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.958100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7859.372559us&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;余弦相似度与余弦距离关系&quot;&gt;余弦相似度与余弦距离关系&lt;/h3&gt;

&lt;p&gt;Supported distances:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Distance&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;parameter&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Equation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Squared L2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;‘l2’&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;d = sum((Ai-Bi)^2)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Inner product&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;‘ip’&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;d = 1.0 - sum(Ai*Bi))&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Cosine similarity&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;‘cosine’&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;d = 1.0 - sum(Ai*Bi) / sqrt(sum(Ai*Ai) * sum(Bi*Bi))&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;参数说明&quot;&gt;参数说明&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;efConstruction：设置得越大，构建图的质量越高，搜索的精度越高，但同时索引的时间变长，推荐范围100-2000&lt;/li&gt;
  &lt;li&gt;efSearch：设置得越大，召回率越高，但同时查询的响应时间变长，推荐范围100-2000，在HNSW，参数ef是efSearch的缩写&lt;/li&gt;
  &lt;li&gt;M：在一定访问内，设置得越大，召回率增加，查询响应时间变短，但同时M增大会导致索引时间增加，推荐范围5-100&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HNSW L2space返回的top@K，是距离最小的K个结果，但是在结果表示的时候，距离是从大到小排序的，所以top@K距离是最小的，top@K-1距离是次之，top@1是距离第K大的。只是结果在表示上逆序了而已，不影响最终的结果。如果要按正常的从小到大来排序，则对top@K的结果做个逆序即可。作者在python的接口里，实现了这种逆序，具体见&lt;a href=&quot;https://github.com/nmslib/hnsw/blob/master/python_bindings/bindings.cpp#L287&quot;&gt;bindings.cpp#L287&lt;/a&gt;，所以python的结果和c++的结果，是逆序的差异。&lt;/p&gt;

&lt;h3 id=&quot;参数详细意义&quot;&gt;参数详细意义&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;M：参数M定义了第0层以及其他层近邻数目，不过实际在处理的时候，第0层设置的近邻数目是2*M。如果要更改第0层以及其他层层近邻数目，在HNSW的源码中进行更改即可。另外需要注意的是，第0层包含了所有的数据点，其他层数据数目由参数mult定义，详细的细节可以参考HNSW论文。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;delaunay_type：检索速度和索引速度可以通过该参数来均衡heuristic。HNSW默认delaunay_type为1，将delaunay_type设置为1可以提高更高的召回率(&amp;gt; 80%)，但同时会使得索引时间变长。因此，对于召回率要求不高的场景，推荐将delaunay_type设置为0。&lt;/li&gt;
  &lt;li&gt;post：post定义了在构建图的时候，对数据所做预处理的数量（以及类型），默认参数设置为0，表示不对数据做预处理，该参数可以设置为1和2（2表示会做更多的后处理）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更详细的参数说明，可以参考&lt;a href=&quot;https://github.com/nmslib/nmslib/blob/9ed3071d0a74156a9559f3347ee751922e4b06e7/python_bindings/parameters.md&quot;&gt;parameters说明&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;demo&quot;&gt;demo&lt;/h3&gt;

&lt;p&gt;小白菜基于局部特征，采用HNSW做过一版实例搜索，详细说明详见&lt;a href=&quot;https://github.com/willard-yuan/cvtk/tree/master/hnsw_sifts_retrieval&quot;&gt;HNSW SIFTs Retrieval&lt;/a&gt;。适用范围：中小规模。理论上，直接基于局部特征索引的方法，做到上千万级别的量级，是没有问题的，成功的例子详见&lt;a href=&quot;http://flickrdemo.videntifier.com/&quot;&gt;videntifier&lt;/a&gt;，&lt;a href=&quot;http://www.videntifier.com/about&quot;&gt;Herwig Lejsek&lt;/a&gt;在设计videntifier系统的时候，使用的是NV-Tree，每一个高维向量只需用6个字节来表示，压缩比是非常大的，(O)PQ折中情况下一般都需要16个字节。关于NV-Tree的详细算法，可以阅读&lt;a href=&quot;http://www.videntifier.com/about&quot;&gt;Herwig Lejsek&lt;/a&gt;的博士论文&lt;a href=&quot;https://en.ru.is/media/skjol-td/PhDHerwig.pdf&quot;&gt;NV-tree: A Scalable Disk-Based High-Dimensional Index&lt;/a&gt;，墙裂推荐一读。&lt;/p&gt;

&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;

&lt;p&gt;在本篇博文里，小白菜对图ANN、基于量化的两类方法分别选取了最具代表性的方法HNSW和OPQ方法进行比较详细的总结，其中由以基于PQ的量化方法在工业界最为实用，基于图的ANN方法，在规模不是特别大但对召回要求非常高的检索场景下，是非常适用的。除此之外，图ANN方法可以和OPQ结合起来适用，来提高OPQ的召回能力，具体可以阅读&lt;a href=&quot;https://arxiv.org/abs/1802.02422&quot;&gt;Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors&lt;/a&gt;和&lt;a href=&quot;https://arxiv.org/abs/1804.09996&quot;&gt;Link and code: Fast indexing with graphs and compact regression codes&lt;/a&gt;这两篇文章。&lt;/p&gt;
</description>
        <pubDate>Sun, 15 Jul 2018 00:00:00 +0800</pubDate>
        <link>https://www.aiforall.pro//blog/2018/07/opq-and-hnsw/</link>
        <guid isPermaLink="true">https://www.aiforall.pro//blog/2018/07/opq-and-hnsw/</guid>
      </item>
    
      <item>
        <title>视觉检索：视频多帧排序</title>
        <description>&lt;h2 id=&quot;背景与问题&quot;&gt;背景与问题&lt;/h2&gt;

&lt;p&gt;每一个视频抽取n帧，n是变化的，有的视频长抽的帧数多，有的视频短抽的帧数相应的也少一些。在索引的时候，将所有视频的帧都索引在一起。对于查询的视频，同样抽取视频帧，假设抽取到了m帧，那么问题来了，对这m帧的查询结果，其排序逻辑该如何设计？&lt;/p&gt;

&lt;h2 id=&quot;多帧相似性度量&quot;&gt;多帧相似性度量&lt;/h2&gt;

&lt;p&gt;对于文章开头提出的问题，可以先对其进行简化，先思考这样一个问题：&lt;strong&gt;两个视频&lt;/strong&gt;，如何度量两个视频的相似性（引申问题：如果校验两个图片或两个视频是重复的）？&lt;/p&gt;

&lt;p&gt;如果两个视频，其提取的特征是视频维度的全局特征，即每个视频最终表示成了一个全局向量表示（比如iDT、C3D等特征），那么度量两个视频间的相似性时，可以直接计算它们之间的余弦相似性或者欧式距离等。这样固然最省事，但在实际应用中，当视频存量上亿，并且还有源源不断的视频添加进来时，显然基于视频维度提特征的方式因为耗费资源太大而无法得以在具备实时性要求的场景中加以应用。因而实际中在对视频内容做相关理解与分析的时候，往往是基于抽帧的方式（如何抽帧以及是否做关键帧检测在此不表）。在获取到视频的帧后，可以在此基础上提取视频维度的特征，或者提取帧级别的特征。&lt;/p&gt;

&lt;p&gt;小白菜简单地总结视频维度特征与帧级别特征（类似全局特征和局部特征）的优劣：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;视频维度特征：考虑了时空维度信息，有利于表述整个视频发生的事件等全局信息，表达的特征更紧凑；&lt;/li&gt;
  &lt;li&gt;帧级别特征：最大的缺陷在于未考虑视频时间维的信息，将帧与帧之间孤立起来了。当然帧级别的特征，其对视频空间信息的特征描述的力度更细一些，在某些场景中，比如校验两个视频是否是重复视频，基于帧级别的特征反而更适用；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一般而言，抽取视频维度的特征通常是非常耗时的。因而，实际面向视频的应用，除非场景应用非常有价值且规模不大可能会使用视频维度的特征，大部分的视频内容理解往往是基于帧级别的特征。现在回到本节开头的问题：两个视频，如何度量两个视频的相似性？这个问题小白菜曾在&lt;a href=&quot;http://yongyuan.name/blog/asymmetry-problem-in-computer-vision.html&quot;&gt;机器视觉：Asymmetry Problem in Computer Vision&lt;/a&gt;中介绍累积最小（最大）距离非对称问题时有涉及过，可以看出，累积最小（最大）距离极其符合该应用场景。因而，对于两个视频，要度量两个视频的相似性，可以采用累积最小（最大）距离，即：&lt;/p&gt;

&lt;p&gt;\begin{equation}
S(X, Y) = \sum_{i=1}^{i=n} \sum_{j=1}^{j=m} d_\min(x_i, y_j)
\end{equation}&lt;/p&gt;

&lt;p&gt;累积最小（最大）距离在应用的时候，可能会出现A视频中的多个帧匹配到B视频中的一帧或交叉匹配的情况（视频序列是时序的）。对于这种情况，在算帧匹配的时候，可以使用动态规划（DP）方法避免出现多帧和一帧匹配或交叉匹配，并确保得到的$S(X,Y)$又是最小的。当然，直接计算累积最小（最大）距离也依然是很好的。实际应用中，这种基于累积最小（最大）距离的多帧相似度度量经受住了应用的考验，而且计算效率比采用DP避免交叉匹配方式更高，并且效果很理想。&lt;/p&gt;

&lt;p&gt;有了两个视频间多帧相似性的度量，将其拓展到一个query视频跟N个视频的相似性排序，就相比比较直观了。下面分别对暴力搜索多帧排序和OPQ多帧排序予以介绍，OPQ多帧排序是暴力搜索多帧排序的继承，只不过在对每帧查询的时候，使用的是OPQ索引，而暴力搜索则是直接query。&lt;/p&gt;

&lt;h2 id=&quot;暴力搜索多帧排序&quot;&gt;暴力搜索多帧排序&lt;/h2&gt;

&lt;p&gt;在不需要实时处理、对召回要求很高并且不是很频繁查询的场景，比如回溯任务中，暴力搜索仍然是一种值得推荐的搜索方法。原因有二：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一是特征性能发挥到最大，从而保证最好的召回率；&lt;/li&gt;
  &lt;li&gt;二是在特征维度不是很高的情况下（几百维，如果是CNN特征且维度较高，比如上千维，可以降到128d或者512d，损失精度通常很小），通过SSE加速，距离的计算也可以算得很快。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于多帧排序，其排序逻辑建立在&lt;strong&gt;两个视频&lt;/strong&gt;多帧相似性度量上，即对于一个query视频，库中的每个视频在与query视频做相似性度量的时候，均应采用上一节指出的&lt;strong&gt;累积最小（最大）距离&lt;/strong&gt;分别计算相似度。在实际计算的时候，这种逐个视频计算的方式可以优化为query视频与N个视频进行内积运算（特征在提取的时候进行了$L2$归一化）的方式，然后对相似性矩阵做排序处理，假设query视频帧数是$m$帧，库中共$N$个视频，每一个视频取$n$帧，则相似性矩阵大小为$m \times (N*n)$，即每一行对应query视频某一帧查询后排序的结果，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/multiframes_reranking1.png&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图中，qF1表示查询视频的第1帧，AF1表示视频A的第1帧，后面以此类推。对该排完序后的相似性矩阵，我们需要对每$i(i = 0,…,m)$行取出视频id对应的最大的值（&lt;strong&gt;因为计算的是余弦相似度&lt;/strong&gt;）作为该帧对应各视频id的结果，这个过程可以用下图来直观的表述出来：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/multiframes_reranking2.png&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，以qF1帧查询为例，由于已经对该帧查询的结果按相似性进行的排序，因而qF1帧查询对A视频查询的结果为AF1，其余以此类推。最后对这m帧经过最大值筛选后的结果，按视频id进行相似性分数的归并，最终得到多帧相似性排序分数，即：&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A视频分数：AF1+AF2+AF1+...+AF3 
B视频分数：BF1+BF1+BF1+...+BF1
C视频分数：CF1+CF3+0.0+...+0.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面假设C视频对于查询视频的每帧，在设置得阈值条件下无返回结果，因而用分数0.0来表示（可以视为将所有的结果在初始化的时候初始化为0）。实际query的时候，对于每一帧的查询结果，不需要将所有的查询结果返回回来，可以通过设置一个相似度阈值只返回一定数目的结果，相似度太小的返回来了也没什么用，徒增计算量和浪费资源，这样处理后排序、筛选、合并的计算量大大降低了。&lt;/p&gt;

&lt;h2 id=&quot;opq多帧排序&quot;&gt;OPQ多帧排序&lt;/h2&gt;

&lt;p&gt;在对实时性有要求的场景，比如上传一个视频，需要从海量的视频库中召回相同或者相似的那些视频，显然暴力搜索多帧排序无法满足要求，需要以某种近似最近邻搜索方法来构建索引。关于近似最近邻搜索方法，在此前的博文如&lt;a href=&quot;http://yongyuan.name/blog/ann-search.html&quot;&gt;图像检索：再叙ANN Search&lt;/a&gt;以及其他的博文中有过很多的介绍，这里仅以OPQ为例来介绍ANN多帧排序方法。&lt;/p&gt;

&lt;p&gt;OPQ多帧排序方法和暴力搜索多帧排序都是基于多帧相似性度量，不同的是，由于OPQ（或PQ）计算的非对称距离，其算出来的值越小，表示两者越相似，而暴力搜索采用的是余弦相似度，其值越大表示两者越相似。因而，对于OPQ多帧排序逻辑，在设计的时候，似乎只需要为排序初始化的分数初始化一个比较合理的值，这里只对返回所有结果做初始化，下面是详细的OPQ多帧排序。&lt;/p&gt;

&lt;p&gt;对于某查询视频queryVid，假设queryVid有m帧$（f_1,f_2,…,f_m）$，对于第i帧查询，在距离阈值小于0.1的条件下，返回了k个结果$(r_1, r_2,…,r_k)$，然后对这m帧的结果按视频id进行最小值选取与合并。举个简单的例子，假设某次查询视频只有3帧，下面是每帧查询返回的结果：&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;查询视频第f1帧返回的结果: A1, B1, E1, C1, D1, B2, E2
查询视频第f2帧返回的结果: B1, C2, D1, C1, E1
查询视频第f3帧返回的结果: A2, C1, B2, D2, E1, D1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;对于查询视频第f1帧返回的结果，假设$A1 \leqslant B1 \leqslant E1 \leqslant C1 \leqslant D1 \leqslant B2 \leqslant E2 \leqslant 1.0 $，后面查询的同理。对上面每帧查询的结果，按视频id取最小值，得下面结果：&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;查询视频第f1帧返回的结果: A1, B1, E1, C1, D1
查询视频第f2帧返回的结果: B1, C2, D1, E1
查询视频第f3帧返回的结果: A2, C1, B2, D2, E1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;对于第f2帧返回的结果，给A视频我们置一个比较大的阈值，比如上面提过的置为1.0来代替，表示第f2帧作为查询时，与A视频相似度差得比较大。之后，便可按视频id对结果进行合并了：&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A视频分数：A1+1.0+A2
B视频分数：B1+B1+B2   
C视频分数：C1+C2+C1
D视频分数：D1+D1+D2
E视频分数：E1+E1+E1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后再对A、B、C、D、E视频上面求和后的分数从小到大排序，得到的便是最终多帧查询排序的结果。整个过程在实现的时候，可以采用&lt;code class=&quot;highlighter-rouge&quot;&gt;partial_sort&lt;/code&gt;方法来完成，关于&lt;code class=&quot;highlighter-rouge&quot;&gt;partial_sort&lt;/code&gt;的用法，可以参考&lt;a href=&quot;http://www.cnblogs.com/qlee/archive/2011/05/25/2057281.html&quot;&gt;理解你的排序操作&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;无论是对于暴力搜索多帧排序、还是OPQ多帧排序，在采用了上述介绍的累积最小（最大）距离的多帧相似度度量后，跟之前采用的基于倒排查询相比，查询效果取得了极大的提升。另外对于OPQ多帧排序的结果，还可以对前top@K的结果做一次重排，能够将累积最小（最大）距离的多帧相似度度量的检索准确率发挥到最大的程度。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;在本篇博文中，小白菜以多帧索引及排序为主题进行了一些讨论，包括了&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;多帧相似性度量&lt;/li&gt;
  &lt;li&gt;暴力搜索多帧排序&lt;/li&gt;
  &lt;li&gt;OPQ多帧排序&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3个方面的内容，这些内容在视频搜索上一般都会涉及。关于多帧索引及排序的问题，学术上能够查询的论文比较少，这里小白菜结合自己的理解以及应用实践总结了小部分的经验，希望对遇到这方面问题的小伙伴有些参考的价值，也欢迎在一线从事CBIR的小伙伴拍砖交流。&lt;/p&gt;
</description>
        <pubDate>Sun, 27 May 2018 00:00:00 +0800</pubDate>
        <link>https://www.aiforall.pro//blog/2018/05/multi-frames-ranking-problem/</link>
        <guid isPermaLink="true">https://www.aiforall.pro//blog/2018/05/multi-frames-ranking-problem/</guid>
      </item>
    
      <item>
        <title>图像检索：图像拷贝检索PHash改进方案</title>
        <description>&lt;p&gt;感知哈希是用来做图像拷贝检索（Copy Detection）最容易操作的一种方法，实际上除了感知哈希、均值哈希，还有很多的从图像本身出发计算出来的图像哈希值，在OpenCV 3.3及其以后的版本中，包含了很多图像哈希的计算方法，具体的可以参考&lt;a href=&quot;https://docs.opencv.org/3.3.1/d4/d93/group__img__hash.html&quot;&gt;The module brings implementations of different image hashing algorithms&lt;/a&gt;，其中各种图像哈希方法对8种不同变化的抗干扰程度，文档中做了一个很好的总结：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://docs.opencv.org/3.3.1/attack_performance.JPG&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从图中可以看到，Phash具备较好的对不同变化的抗干扰性，因为在&lt;strong&gt;一般要求不高的图像拷贝检索&lt;/strong&gt;场景中，应用得较多。下面小白菜就PHash的原理（计算步骤）、在使用中存在的问题以及改进方案做一个记录与总结。&lt;/p&gt;

&lt;h3 id=&quot;phash原理&quot;&gt;PHash原理&lt;/h3&gt;

&lt;p&gt;感知哈希（Perceptual Hash, PHash）比均值哈希要稳健，PHash使用DCT将图像由空域转为频域，并对频域的低频成分进行散列化。PHash算法可分为以下几个步骤：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;将图片resize到32*32最好，这样可以简化DCT计算；&lt;/li&gt;
  &lt;li&gt;将彩色图像转为灰度图像；&lt;/li&gt;
  &lt;li&gt;计算DCT，使用32*32的DCT变换；&lt;/li&gt;
  &lt;li&gt;DCT的结果是32*32的矩阵，只要保留左上角的8*8矩阵就可以了，因为这部分呈现了图片中的最低频率；&lt;/li&gt;
  &lt;li&gt;计算DCT的平均值；&lt;/li&gt;
  &lt;li&gt;根据8*8的DCT矩阵来与平均值进行比较，大于平均值的为1，小于的为0。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：在保留左上角的8*8矩阵的时候，不一定非得是最左上角，可以往右下移一个或几个像素。下面是采用PHash做拷贝检索结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/okcase_phash.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，对于上面查询，PHash能够获得很好的拷贝检索准确率，但是PHash除了上面图表所示的对椒盐噪声、旋转几乎不具有抗干扰特性外，还有其他的方面的一些局限。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PHash的局限&lt;/strong&gt;：PHash为了容忍图像的一些形变而只取了图像的低频部分，从而造成了特征捕获不到图像的细节部分，使得对于纯色或者近似纯色图像在做查重的时候并不是很理想。具体地说，由于PHash取的是dct的左上角部分，属于低频成分，也就是关注的是图像的大致外形，没有关注细节，所以对于纯色图片或者近似纯色图片，就没法捕获到它的轮廓细节，导致对纯色图像或者近似纯色图片的查重准确率不高。一种改进的方式是：把这个取的部分往右下移，这样就可以获得图像的轮廓细节，纯色图像或者近似纯色图片查重准确率就会提升，坏处就是轮廓细节取多了，容忍细节变化就小了，导致图像容忍的形变变小，但是这种思路是值得借鉴的。&lt;/p&gt;

&lt;h3 id=&quot;phash特性&quot;&gt;PHash特性&lt;/h3&gt;

&lt;p&gt;PHash对噪声、模糊、jpeg压缩等具备较好的不变性，此外，在实际应用中还有另一类变换也是值得非常关注的，即对图像做水平镜像操作。&lt;strong&gt;PHash对镜像不具备不变性&lt;/strong&gt;，可以通过一个简单的实验予以验证。&lt;/p&gt;

&lt;p&gt;实验过程：测试了两对图片，每一对图片包含图片自身，已经其对应的镜像图片（图片对的大小是一样的，说明为jpeg压缩影响），分别计算图片对之间的距离，一对算出来的距离是35，一对算出来的是33。说明PHash对镜像无召回能力。&lt;/p&gt;

&lt;p&gt;既然谈到了图像的镜像变换，我们不妨对PHash、基于SIFT特征的Fisher Vector以及DL相似特征对镜像变换是否有不变形做一个整理：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;特征&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;形变&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;是否抗干扰&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;备注&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;PHash特征&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;镜像&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;否&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;见上面说的验证过程&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FV特征&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;镜像&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;很弱&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SIFT对镜像不具备不变性，故FV对镜像召回能力很弱，具体参考论文&lt;a href=&quot;http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=6336821&quot;&gt;Flip-Invariant SIFT for Copy and Object Detection&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;DL相似特征&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;镜像&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;是&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;由于DL相似特征在训练的时候，数据增强里面包含镜像，DL相似特征对镜像具备旋转特性，检索显示的top@K里面，可以找回镜像的视频，见结果&lt;a href=&quot;http://owtbv2q93.bkt.clouddn.com/note/similarity_flip.png&quot;&gt;镜像检索结果&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;phash改进方案&quot;&gt;PHash改进方案&lt;/h3&gt;

&lt;p&gt;在前面已经提到了PHash对于纯色或近似纯色图像做拷贝检索存在的缺陷，当DCT进行散列化时如果选取的DCT的频率过低，则对纯色或近似纯色图像的拷贝检索存在badcase，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/pureColor_phash.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上图可以看到，由于查询图像是接近纯色的图片，导致取得DCT的低频只能捕获到图像的大致外观，因而很多接近纯色的图片到排在了前面。对这类case的改进优化，我们知道无论是取低频还是高频部分做散列化都不合适，如果只取高频，则会影响正常图片的召回。因此，比较容易想到的一种改进方式是：对于正常的图片，只需采用低频DCT哈希值做排序；对于纯色或近似纯色图像，先用低频DCT哈希值检索排序，然后再用高频DCT哈希值检索再做重排。这种改进方式的好处是显而易见的，对于每一张图片，只需要额外增加64个比特位的存储空间，并且不用对整个拷贝检索的架构做很大的调整，我们所要做的就是再计算一下高频DCT的哈希值，并且增加一个对纯色或接近纯色的检索服务，就能使PHash在图像拷贝检索上获得较大的精度提升，同时又不至于较大的减少召回。&lt;/p&gt;

&lt;p&gt;对于图像纯色或接近纯色的检索，小白菜以为应该做得轻巧简洁，因为本身PHash做拷贝检索就是一个很轻量的服务，如果图像纯色或接近纯色的检索做的过重，比如用DL对图像纯色与非纯色进行分类，就失去了用PHash做拷贝检索的意义，另外采用还需消耗大量的GPU，因而图像纯色或接近纯色的服务越轻巧越好。下面小白菜提供的一个极轻量的图像纯色或接近纯色的检索方法：&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imGray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;123.jpg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV_LOAD_IMAGE_GRAYSCALE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;histSize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;histRange&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calcHist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imGray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;histSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;histRange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sortIdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV_SORT_EVERY_COLUMN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV_SORT_DESCENDING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV_SORT_EVERY_COLUMN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV_SORT_DESCENDING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    
&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxFre&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;secFre&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;allFre&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    
&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ratio1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxFre&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;secFre&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;allFre&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;  
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ratio1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.51&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;pure image&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;根据灰度测试的结果，在阈值为0.51时，粗略评估对于纯色或接近纯色图像的召回率至少在85%以上，准确率在90%以上，检测速度在10ms左右。这里，对于召回率和准确率的要求是，召回越高越好，对准确率的要求可以相对低一点，因为我们的目的是要改善纯色或接近纯色图像的拷贝检索的准确率，可以小幅牺牲点非纯色图像拷贝检索的召回。&lt;/p&gt;

&lt;h3 id=&quot;改善性能验证&quot;&gt;改善性能验证&lt;/h3&gt;

&lt;p&gt;按上述所提的改进方案重排后，即：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于一个查询，先使用低频分量DCT哈希值进行排序；&lt;/li&gt;
  &lt;li&gt;对查询图像进行纯色或者近似纯色图像检测，如果不是纯色或者近似纯色图像，当前排序结果为最终拷贝检索排序结果；&lt;/li&gt;
  &lt;li&gt;如果是纯色或者近似纯色图像，使用高频DCT哈希值对初排结果进行重新排序，对重排序结果只保留汉明距离只小于等于某一阈值的那些结果，将其作为最终排序结果；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是上面查询图像采用该改进方案重排后的一个结果，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/improved_dct.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上面可以看到，经过重排后，对于纯色或者接近纯色图像的拷贝检索，结果有了很大的提升，实际中测试了很多的case，发现都能够获得很好的改善。&lt;/p&gt;

&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;

&lt;p&gt;在本篇文章中，小白菜就PHash的原理、存在的缺陷以及改进的方案做了详细的总结，这个问题的存在以及想到的解决方法并不是凭空产生和获得的，而是实际应用中确确实实会存在这样或那样的问题，需要不断从原理上推敲，然后反复进行实验。当然对PHash的改进应该有非常多，这种改进方案不一定是最好的，但是可以值得借鉴，希望对有需要的同学有所帮助或者启发。&lt;/p&gt;
</description>
        <pubDate>Fri, 16 Mar 2018 00:00:00 +0800</pubDate>
        <link>https://www.aiforall.pro//blog/2018/03/improve-phash-for-copy-detection/</link>
        <guid isPermaLink="true">https://www.aiforall.pro//blog/2018/03/improve-phash-for-copy-detection/</guid>
      </item>
    
      <item>
        <title>机器视觉：Asymmetry Problem in Computer Vision</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;自然法则无时不刻不给予着人类以对称性的恩惠，从一片树叶到人类自身，其形态都是对称的。对称性的特性，大大减轻了人类的记忆和认知负担。然而，弱相互作用中互为镜像的物质的运动不对称却暗藏着自然法则对非对称性的偏爱。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在计算机视觉中，对称性是一个很好的先验，如果某一个特定的物体具备对称性的话，通过引入对称性可以提升系统的精度。常见的对称性包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;物体本身具备对称性，且这种对称性不容易受大视角变化的影响。主要应用场景为在训练诸如检测模型的时候，可以将这一信息加入到训练样本的扩增上；&lt;/li&gt;
  &lt;li&gt;相似性度量具备对称性。这种对称性常体现在设计的相似性度量准则上，A与B计算出的相似性和B与A得到的相似性是一样。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们的大脑深受对称性法则的影响，所以对于第一种情况，我们可以非常直观的将这一法则应用到我们的视觉模型训练中，但是对于第二种情况，特别是当相似性度量由非对称性在对称性上转化而来的时候，我们的第一意识却频频出错。计算机视觉中非对称现象和非对称相似性度量如此干扰我们的第一意识，以至于小白菜对这个问题曾做出过若干的思考，下面是小白菜结合自己的一些思考和经验做的总结和整理。&lt;/p&gt;

&lt;h3 id=&quot;局部特征匹配中的非对称问题&quot;&gt;局部特征匹配中的非对称问题&lt;/h3&gt;

&lt;p&gt;在用局部特征进行匹配的时候，比如SIFT，用A图匹配B图和用B图匹配A图，得到的匹配结果是不一样的，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/sift_matching_diff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这匹配的过程中，所有的度量方式比如最近邻、次近邻、几何校验等都是具备对称性的，所以很容易给我们造成一种错觉就是他们的匹配结果应该是一样的。这里面造成匹配结果不对称的根本因素在于&lt;strong&gt;A图和B图它们的局部特征数目是不相等的&lt;/strong&gt;，比如A有500个局部特征，B有800个局部特征，用500个局部特征去匹配800局部个特征和用800个局部特征去匹配500个局部特征，势必造成匹配的结果不一样。一个鲁棒的匹配算法，应该在用A匹配B和用B匹配A时都能获得比较好的匹配结果，以避免单一结果匹配较好的情形，像下面的匹配算法就不是一种好的匹配算法：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/false_sift_matching.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在设计匹配算法的时候，我们应&lt;strong&gt;避免我们的算法出现单一匹配好的情形，以提高匹配的鲁棒性&lt;/strong&gt;。如需获取上图匹配结果，可以访问&lt;a href=&quot;https://github.com/willard-yuan/covdet.git&quot;&gt;covdet&lt;/a&gt;获取。&lt;/p&gt;

&lt;h3 id=&quot;logo识别中的非对称问题&quot;&gt;Logo识别中的非对称问题&lt;/h3&gt;

&lt;p&gt;Logo的识别问题，可以分为两类:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基于检测的方式&lt;/li&gt;
  &lt;li&gt;基于检索的方式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基于检测方式的缺点在于新的Logo样式来临时，会面临训练样本如何获取以及人工标注的问题，虽然可以自动通过往图片上贴Logo的方式来构造训练样本，但这种方式容易造成过拟合；基于检索的方式的优点在于，它不会面临训练样本获取和标注的问题，能够以较快的速度响应新Logo检测的请求，但是基于Logo检索的方式，面临一个很大的问题就是尺寸不对称性。&lt;/p&gt;

&lt;p&gt;这种尺寸不对称性体现在：对于待检测的图片，Logo所占的区域是很小的，通常区域面积占总体面积比的5%都不到，这样就导致提取的Logo区域的特征（比如局部特征）被非Logo区域的特征给“淹没”掉，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/logo_example.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图这个图选取得不是很合适，因为背景比较干净，Logo区域的特征还是其了比较大的作用。对于一般的情况，由于Logo区域的特征被非Logo区域的特征给淹没掉，从而使得在Logo库里检索的时候，在top@K（K取得比较小）里面比较难以检索到相关的Logo，而且即便是做重排，也比较难以将最相关的Logo排到最前面。&lt;/p&gt;

&lt;p&gt;针对这种由于尺寸不对称性造成的有用特征（信号）被无用信号淹没的问题，很难在不对检索精度造成较大影响的前提下找到比较有效的解决办法。如果要剔除无用信号造成的干扰，一种比较好的方法是先进行粗略的检测，这一步不要求检测的准确率很高，只要保证召回率很高即可，然后可以根据定位到的框提取特征，再进行检索以及校验。这种方式由于剔除了非相关区域特征的干扰，所以准确率和召回率通常能够得到较好的保证，唯一不足的是，引入了检测，而检测势必要求对数据进行标注（半自动）。不过总体来说，这仍然是一种非常不错的方法。&lt;/p&gt;

&lt;h3 id=&quot;pq中的非对称距离&quot;&gt;PQ中的非对称距离&lt;/h3&gt;

&lt;p&gt;在此前的文章&lt;a href=&quot;http://yongyuan.name/blog/ann-search.html&quot;&gt;图像检索：再叙ANN Search&lt;/a&gt;中，小白菜曾对PQ做过比较详细的介绍，这里对PQ中非对称距离的计算做一详述。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/pq_search.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，非对称距离计算方式（红框标示）在计算查询向量$x$到库中某一样本$y$之间的距离时，并不需要对查询向量$x$自身进行量化，而是直接计算查询向量$x$到量化了的$y$之间的距离，这种距离计算方式可以确保非距离计算方式以更大的概率保证计算的距离更接近于真实的距离（与对称距离计算方式相比较），而且这种方式比对称距离计算方式在实施的过程中，来得更直接，因为我们不需要对查询向量进行量化，而是直接计算到对应子段之间的距离，然后采用查表的方式获取到查询向量到库中所有样本的距离。这种通过查表的方式，是PQ能够加速距离计算的核心。&lt;/p&gt;

&lt;p&gt;非对称性的应用，在这里展示了它有利的一面，通过将&lt;strong&gt;非对称性延展到相似性度量上，可以进一步缩小量化造成的损失&lt;/strong&gt;。既然谈到了PQ的思想，我们还可以对PQ的改进做一下的延拓。&lt;/p&gt;

&lt;h4 id=&quot;polysemous-codes&quot;&gt;Polysemous Codes&lt;/h4&gt;

&lt;p&gt;PQ的改进版本很多，比如&lt;a href=&quot;kaiminghe.com/publications/pami13opq.pdf&quot;&gt;OPQ&lt;/a&gt;, Jegou等人又在PQ的基础上对PQ计算距离的过程中做了进一步加速，提出了&lt;a href=&quot;https://arxiv.org/abs/1609.01882&quot;&gt;Polysemous Codes&lt;/a&gt;（中译为“一词多义编码”，为何叫Polysemous Codes，容小白菜慢慢道来)。&lt;/p&gt;

&lt;p&gt;前面已经提到，即便是采用查表的方式，仍然还是要查表挨个对库中的每个样本计算到查询向量之间的距离，这种距离能不能转换成汉明距离的计算？正如上图所示的，对于每个向量，我们可以知道它对应的量化索引编码，如果这个量化索引编码本身就是一种汉明编码，那么我们就可以直接用通过计算汉明距离来得到粗排序的结果，然后再对topK的结果计算ADC距离，Polysemous Codes的动机正是如此，实际上Polysemous Codes不仅充当了量化索引编码，还充当了一个快速过滤的作用。&lt;/p&gt;

&lt;p&gt;应该说，对于所有的相似性度量距离，汉明距离计算从效率上来讲，是最快的。在faiss的项目wiki的&lt;a href=&quot;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes-(composite)&quot;&gt;re-filtering PQ codes with polysemous codes&lt;/a&gt;有结论：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is about 6x faster to compare codes with Hamming distances than to use a product quantizer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;也就是计算一次PQ的距离，跟计算一次汉明距离计算相比，会慢6倍左右，说明如果能把PQ编码赋予汉明编码的意义的话，距离的计算会提升6倍，这个提升还是非常巨大的。Polysemous Codes的实现已在Faiss中&lt;a href=&quot;https://github.com/facebookresearch/faiss/blob/master/IndexPQ.h#L67&quot;&gt;Polysemous Codes Implementation&lt;/a&gt;。下图解释一下“Polysemous Codes”的“Polysemous”，即一词多义：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/polysemous_codes.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Polysemous codes are compact representations of vectors that can be comparedeither with product quantization (222M distance evaluations per second per core for 8-byte codes) or as binary codes (1.19G distances per second). To obtain this property, we optimize the assignment of quantization indexes to bits such that closest centroidshave a small Hamming distance. 【摘自&lt;a href=&quot;https://arxiv.org/abs/1609.01882&quot;&gt;Polysemous Codes&lt;/a&gt;】&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在标准的PQ编码中，编码仅指代对应的量化索引编码，即由哪个类中心来近似该子段向量，比如上面的左图，4位的二进制编码仅表示类中心编码（编号），除此之外，无任何其他的意义；而在Polysemous Codes中，如右图所示，它不仅能表示类中心的编码（编号），而且它还是一种汉明编码，从这里可以看到，该编码包含了两种特性，我们既可以计算PQ距离，又可以计算汉明距离，因而该编码被称为“Polysemous Codes”是非常妥帖的。由于计算汉明距离更高效，而Polysemous Codes又是一种汉明码，我们可以先用汉明距离进行粗排序，在采用PQ距离进行重排。&lt;/p&gt;

&lt;p&gt;Polysemous Codes所拥有的这两种特性，不能不感叹它是极其优雅美丽的。&lt;/p&gt;

&lt;h3 id=&quot;累计最小最大距离非对称问题&quot;&gt;累计最小（最大）距离非对称问题&lt;/h3&gt;

&lt;p&gt;在度量两个集合的相似性的时候，累计最小（最大）距离是一个比较好用的相似性度量距离。假设两个集合分别为$X = \lbrace x_t, i=1 \dots n \rbrace$和$Y = \lbrace y_t, j=1 \dots m \rbrace$，则$X$和$Y$集合的相似性可以通过累计最小（最大）距离来度量，即：&lt;/p&gt;

&lt;p&gt;\begin{equation}
S(X, Y) = \sum_{i=1}^{i=n} \sum_{j=1}^{j=m} d_\min(x_i, y_j)
\end{equation}&lt;/p&gt;

&lt;p&gt;至于$d$选取何种距离，我们可以根据自己的应用场景来定夺，小白菜自己一般喜欢使用余弦相似度，因为该距离的计算最终可以转换成内积。累计最小（最大）距离应用的场景这里可以列举一二：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;局部特征构成的两个集合，这里的局部特征不限于传统的局部特征，还可以是CNN构造的局部特征；&lt;/li&gt;
  &lt;li&gt;两个视频序列分别对视频帧提取全局特征，构成的两个视频帧特征集合，使用累计最小（最大）距离我们可以得到两个视频的相似性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;累计最小（最大）距离通常也是一种非对称性距离，在两个集合样本数量相差得比较大时，这种非对称性表现得非常明显。以上面所说的第二个例子为例，当两个视频由于某种不是很合理的取帧方式，导致两个视频取的帧数目相差比较大时，计算出的$S(X, Y)$和$S(Y, X)$会相差得很大，必然会导致其中的一个得到的相似度比较小。对于这种情形，在此相似性度量方式下，并没有特别好的办法来改善，所以在取帧的时候，尽量使两者的数目相当。&lt;/p&gt;

&lt;p&gt;总体来说，累计最小（最大）距离对于度量两个集合的相似性是个不错的距离度量。下面讲讲对这个距离度量的计算速度优化问题。比如，我们要计算$X$和$Y$两个集合的相似性，并且每个集合有1000个元素，我们是要一个一个遍历计算找最小值然后相加吗？&lt;/p&gt;

&lt;p&gt;显然这种计算方式太慢。前面提到过，在对$d$相似性度量的选取上，小白菜喜欢使用余弦相似性，矩阵乘法使得我们可以避免掉挨个遍历循环，通过矩阵相乘得到$X$和$Y$中各个样本与样本之间的相似性后，我们可以对相似性矩阵进行排序，然后求和相加即可得到$S(X, Y)$，即$X$和$Y$之间的相似性。&lt;/p&gt;

&lt;p&gt;总结一下，在本篇博文中，分别从下面4个方面对计算机视觉中的非对称问题进行了探讨：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;局部特征匹配中的非对称问题&lt;/li&gt;
  &lt;li&gt;Logo识别中的非对称问题&lt;/li&gt;
  &lt;li&gt;PQ中的非对称距离&lt;/li&gt;
  &lt;li&gt;累计最小（最大）距离非对称问题&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;中间穿插了对PQ的拓展Polysemous Codes的拓展。后面如遇其他计算机视觉中的非对称性问题，会同步到这里。&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Dec 2017 00:00:00 +0800</pubDate>
        <link>https://www.aiforall.pro//blog/2017/12/asymmetry-problem-in-computer-vision/</link>
        <guid isPermaLink="true">https://www.aiforall.pro//blog/2017/12/asymmetry-problem-in-computer-vision/</guid>
      </item>
    
      <item>
        <title>图像检索：Fisher Information Matrix and Fisher Kernel</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;http://wiki.mbalib.com/wiki/%E7%BD%97%E7%BA%B3%E5%BE%B7%C2%B7%E8%B4%B9%E9%9B%AA&quot;&gt;罗纳德·费雪&lt;/a&gt;（Sir Ronald Aylmer Fisher, FRS，1890.2.17－1962.7.29)，现代统计学与现代演化论的奠基者之一，安德斯·哈尔德称他是“一位几乎独自建立现代统计科学的天才”，理查·道金斯则认为他是“达尔文最伟大的继承者”。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/ronald_aylmer_fisher.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;局部特征作为一种强鲁棒性的特征，其与全局特征构成了CV领域图像内容描述的基础。相比于全局特征，局部特征往往在对低层共有模式的表达上可以做到更细的粒度（关于局部和全局在视觉认知上的作用机制，强烈推荐阅读&lt;a href=&quot;https://en.wikipedia.org/wiki/Scale_space&quot;&gt;尺度空间理论&lt;/a&gt;），但同时也引发了新的问题，即&lt;strong&gt;特征处理效率低、存储大等方面的问题&lt;/strong&gt;。因而需要将局部特征经过某种编码方式，最终表示成一种紧凑的全局特征表示。&lt;/p&gt;

&lt;p&gt;Fisher Vector作为连接单向连接局部特征到全局表示的三大特征编码方法之一（另外两种编码方式见&lt;a href=&quot;http://yongyuan.name/blog/CBIR-BoF-VLAD-FV.html&quot;&gt;图像检索：BoF、VLAD、FV三剑客&lt;/a&gt;），无论是在学术研究领域还是在工业实际应用上，都具有很重要的地位。下面内容是小白菜对Fisher Vector中的Fisher信息矩和Fisher核的数学推导及对应物理意义的总结整理。&lt;/p&gt;

&lt;h2 id=&quot;fisher信息矩和fisher核&quot;&gt;Fisher信息矩和Fisher核&lt;/h2&gt;

&lt;p&gt;在空间$\chi$中，某样本$X$存在$T$个观测量，记为$X = \lbrace x_t, t=1 \dots T \rbrace$。对应到图像上，样本$X$为图像$I$提取到的$T$个$D$维的局部描述子，比如SIFT。设$\mu_\lambda$为概率密度函数，该函数包含有$M$个参数，即$\lambda = [\lambda_1, \dots, \lambda_M]$。根据生成式原理，空间$\chi$中的元素$X$可以由概率密度函数进行建模。在统计学上，分数函数（score funciton）可以由对数似然函数的梯度给出，即：&lt;/p&gt;

&lt;p&gt;\begin{equation}
G^X_\lambda = \nabla log \mu_\lambda(X)
\label{sfun}
\end{equation}&lt;/p&gt;

&lt;p&gt;上式对数函数的梯度，在数学形式上为对数似然函数的一阶偏导，它描述了每一个参数$\lambda_i$对该生成式过程的贡献度，换言之，该分数函数$G^X_\lambda$描述了生成式模型$\mu_\lambda$为了更好的拟合数据$X$，该模型中的参数需要做怎样的调整。又因为$G^X_\lambda \in R^M $是一个维度为$M$维的向量，所以该分数函数的维度仅依赖于$\lambda $中参数的数目$M$, 而于观测样本的数目$T$无关。此外，一般情况下，该分数函数的期望$E[ G^X_\lambda ] = 0 $，这一点对于下面讲到的Fisher信息矩物理意义的得到非常重要。&lt;/p&gt;

&lt;p&gt;根据信息几何理论，含参分布$\Gamma = \lbrace \mu_\lambda, \lambda \in \Lambda \rbrace$可以视为一个黎曼流形$M_\Lambda$，其局部度量方式可以由Fisher信息矩(Fisher Information Matrix, FIM）$F_\lambda \in R^{M \times M}$:&lt;/p&gt;

&lt;p&gt;\begin{equation}
F_{\lambda} = E_{ x \sim \mu_\lambda } [ G^X_\lambda (G^X_\lambda)^T ]
\label{fim}
\end{equation}&lt;/p&gt;

&lt;p&gt;从上式可以看到，Fisher信息矩是分数函数的二阶矩。在一般条件下很容易证明（注意$E[ G^X_\lambda ] = 0 $）：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
F_{\lambda} &amp;= E_{ x \sim \mu_\lambda } [ G^X_\lambda (G^X_\lambda)^T ] \\\\
            &amp;= E[(G^X_\lambda)^2] \\\\
            &amp;= E[(G^X_\lambda)^2] - E[ G^X_\lambda ]^2 \\\\
            &amp;= Var[G^X_\lambda]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;从上式可以看到，Fisher信息矩是用来估计最大似然估计（Maximum Likelihood Estimate, MLE）的方程的方差。它直观的表述就是，在独立性假设的条件下，随着收集的观测数据越来越多，这个方差由于是一个相加的形式，因而Fisher信息矩也就变的越来越大，也就表明得到的信息越来越多。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注：此处引用了&lt;a href=&quot;https://www.zhihu.com/question/26561604&quot;&gt;fisher information 的直观意义是什么?&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于两组不同的观察样本$X$和$Y$，Jaakkola和Haussler提出了使用Fisher核来度量它们之间的相似性，其数学表达形式为：&lt;/p&gt;

&lt;p&gt;\begin{equation}
K_FK(X, Y) = (G^X_\lambda)^T F_\lambda^{-1} G^X_\lambda
\label{fk}
\end{equation}&lt;/p&gt;

&lt;p&gt;又因为$F_{\lambda}$是半正定的，所以其逆矩阵是存在的。使用cholesky分解可以得到$F_\lambda^{-1} = (L_\lambda)^T L_\lambda$，上式可以写成内积的表示形式：&lt;/p&gt;

&lt;p&gt;\begin{equation}
K_FK(X, Y) = (\wp^X_\lambda)^T \wp^Y_\lambda
\label{fk1}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中,
\begin{equation}
\wp^X_\lambda = L_\lambda G^X_\lambda = L_\lambda \nabla log \mu_\lambda(X)
\label{wp_lambda}
\end{equation}&lt;/p&gt;

&lt;p&gt;上式是$L_\lambda$对$G^X_\lambda$的归一化，我们将$\wp^X_\lambda$称为Fisher向量，该Fisher向量$\wp^X_\lambda$等于梯度向量$G^X_\lambda$的维度，又由于$G^X_\lambda$的维度仅与概率密度函数的参数数目$M$有关，所以空间$\chi$中任意样本$X$的$T$个观测量最终都可以表示成一固定维度的向量。通过使用$\wp^X_\lambda$算子，使得非线性核相似性度量问题转化为线性问题。这种变换带来的一个明显的优势是，在分类的时候可以采用更高效的线性分类器。&lt;/p&gt;
</description>
        <pubDate>Mon, 02 Oct 2017 00:00:00 +0800</pubDate>
        <link>https://www.aiforall.pro//blog/2017/10/fim-fisher-kernel/</link>
        <guid isPermaLink="true">https://www.aiforall.pro//blog/2017/10/fim-fisher-kernel/</guid>
      </item>
    
      <item>
        <title>深度学习：Neural Network Layers Understanding</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;我想做又应该做的事，都会做到；我想做却不应做的事，都会戒掉。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;inner-product-layer&quot;&gt;Inner Product Layer&lt;/h2&gt;

&lt;p&gt;Inner Product Layer即全连接层，对于IP层的理解，可以简单的将其视为矩阵&lt;code class=&quot;highlighter-rouge&quot;&gt;1*N&lt;/code&gt;和矩阵&lt;code class=&quot;highlighter-rouge&quot;&gt;N*M&lt;/code&gt;相乘后得到&lt;code class=&quot;highlighter-rouge&quot;&gt;1*M&lt;/code&gt;的维度向量。&lt;/p&gt;

&lt;p&gt;举个简单的例子，比如输入全连接层的是一个&lt;code class=&quot;highlighter-rouge&quot;&gt;3*56*56&lt;/code&gt;维度的数据，假设未知的权重维度为&lt;code class=&quot;highlighter-rouge&quot;&gt;N*M&lt;/code&gt;，假设全连接层的输出为&lt;code class=&quot;highlighter-rouge&quot;&gt;num_ouput = 4096&lt;/code&gt;，为了计算全连接层的输出，全连接层会将输入的数据&lt;code class=&quot;highlighter-rouge&quot;&gt;3*56*56&lt;/code&gt; reshape 成为&lt;code class=&quot;highlighter-rouge&quot;&gt;1*N&lt;/code&gt;的形式，即&lt;code class=&quot;highlighter-rouge&quot;&gt;1x(56x56x3) = 1x9408&lt;/code&gt;，所以：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;N = 9408
M = num_ouput = 4096
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;由此，我们做了一个&lt;code class=&quot;highlighter-rouge&quot;&gt;(1x9408)&lt;/code&gt;矩阵和&lt;code class=&quot;highlighter-rouge&quot;&gt;(9408x4096)&lt;/code&gt;矩阵的乘法。如果&lt;code class=&quot;highlighter-rouge&quot;&gt;num_output&lt;/code&gt;的值改变成为100，则做的是一个&lt;code class=&quot;highlighter-rouge&quot;&gt;(1x9408)&lt;/code&gt;矩阵和&lt;code class=&quot;highlighter-rouge&quot;&gt;(9408x100)&lt;/code&gt;矩阵的乘法。&lt;strong&gt;Inner Product layer（常被称为全连接层）将输入视为一个vector，输出也是一个vector（height和width被设为1）&lt;/strong&gt;。下面是IP层的示意图&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/fcgemm_corrected.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;图片摘自&lt;a href=&quot;https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/&quot;&gt;Why GEMM is at the heart of deep learning&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;增大&lt;code class=&quot;highlighter-rouge&quot;&gt;num_output&lt;/code&gt;会使得模型需要学习的权重参数增加。IP层一个典型的例子：&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool2&quot;
  top: &quot;ip1&quot;
  # learning rate and decay multipliers for the weights
  param {
    lr_mult: 1
  }
  # learning rate and decay multipliers for the biases
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;有了上面对IP层的理解，对&lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/src/caffe/layers/inner_product_layer.cpp&quot;&gt;caffe inner_product_layer.cpp&lt;/a&gt;中Forward的理解就比较自然了。下面是Caffe的IP层在CPU上的实现：&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;InnerProductLayer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Forward_cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Blob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;*&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Blob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;*&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottom_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutable_cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blobs_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;caffe_cpu_gemm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CblasNoTrans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transpose_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CblasNoTrans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CblasTrans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;M_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;bottom_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_term_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;caffe_cpu_gemm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CblasNoTrans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CblasNoTrans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bias_multiplier_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blobs_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上面完成矩阵与矩阵相乘的函数是&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe_cpu_gemm&amp;lt;Dtype&amp;gt;&lt;/code&gt;（见&lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/src/caffe/util/math_functions.cpp&quot;&gt;math_functions.cpp&lt;/a&gt;)，&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe_cpu_gemm&lt;/code&gt;函数矩阵相乘的具体数学表示形式为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
   C=alpha*TransA(A)*TransB(B) + beta*C\\\\
\end{equation}&lt;/script&gt;

&lt;p&gt;上式中&lt;code class=&quot;highlighter-rouge&quot;&gt;TransX&lt;/code&gt;是对&lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;做的一种矩阵变换，比如转置、共轭等，具体是&lt;code class=&quot;highlighter-rouge&quot;&gt;cblas.h&lt;/code&gt;中定义的为枚举类型。在&lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/src/caffe/util/math_functions.cpp&quot;&gt;math_functions.cpp&lt;/a&gt;中，除了定义矩阵与矩阵相乘的&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe_cpu_gemm&lt;/code&gt;外，还定义了矩阵与向量的相乘，具体的函数为&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe_cpu_gemv&lt;/code&gt;，其数学表示形式为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
   C=alpha*TransA(A)*y + beta*y\\\\
\end{equation}&lt;/script&gt;

&lt;p&gt;上面表达式中，&lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;是向量，不是标量。&lt;/p&gt;

&lt;h4 id=&quot;参考&quot;&gt;参考&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/&quot;&gt;Why GEMM is at the heart of deep learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/35788873/what-is-the-output-of-fully-connected-layer-in-cnn&quot;&gt;What is the output of fully connected layer in CNN?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/seven_first/article/details/47378697&quot;&gt;caffe_cpu_gemm函数&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/u011762313/article/details/47361571&quot;&gt;Caffe学习：Layers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://caffe.berkeleyvision.org/tutorial/layers.html&quot;&gt;Caffe Layers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;gemm&quot;&gt;GEMM&lt;/h2&gt;

&lt;p&gt;在上面的IP层中，我们已经涉及到了GEMM的知识，在这一小节里面，不妨对该知识点做一个延伸。&lt;/p&gt;

&lt;p&gt;GEMM是BLAS (Basic Linear Algebra Subprograms)库的一部分，该库在1979年首次创建。为什么GEMM在深度学习中如此重要呢？我们可以先来看一个图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/gemm_cup_gpu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;图片摘自&lt;a href=&quot;http://daggerfs.com/&quot;&gt;Yangqing Jia&lt;/a&gt;的&lt;a href=&quot;http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-93.pdf&quot;&gt;thesis&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上图是采用AlexNet对CNN网络中不同layer GPU和CPU的时间消耗，从更底层的实现可以看到CNN网络的主要时间消耗用在了FC (for fully-connected)和Conv (for convolution)，而FC和Conv在实现上都将其转为了矩阵相乘的形式。举个例子：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/cnn_gemm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;图片摘自&lt;a href=&quot;http://arxiv.org/pdf/1410.0759.pdf&quot;&gt;cuDNN: Efficient Primitives for Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面Conv在Caffe中具体实现的时候，会将每一个小的patch拉成一个向量，很多patch构成的向量会构成一个大的矩阵，同样的对于多个卷积核展成一个矩阵形式，从而将图像的卷积转成了矩阵与矩阵的相乘（更形象化的解释参阅&lt;a href=&quot;https://www.zhihu.com/question/28385679/answer/44297845&quot;&gt;在 Caffe 中如何计算卷积？&lt;/a&gt;）。上面可以看到在FC和Conv上消耗的时间GPU占95%，CPU上占89%。因而GEMM的实现高效与否对于整个网络的效率有很大的影响。&lt;/p&gt;

&lt;p&gt;那么什么是GEMM呢？GEMM的全称是GEneral Matrix to Matrix Multiplication，正如其字面意思所表达的，GEMM即表示两个输入矩阵进行相乘，得到一个输出的矩阵。两个矩阵在进行相乘的时候，通常会进行百万次的浮点运算。对于一个典型网络中的某一层，比如一个256 row&lt;em&gt;1152 column的矩阵和一个1152 row&lt;/em&gt;192 column的矩阵，二者相乘57 million (256 x 1152 x 192)的浮点运算。因而，通常我们看到的情形是，一个网络在处理一帧的时候，需要几十亿的FLOPs（Floating-point operations per second，每秒浮点计算）。&lt;/p&gt;

&lt;p&gt;既然知道了GEMM是限制整个网络时间消耗的主要部分，那么我们是不是可以对GEMM做优化调整呢？答案是否定的，GEMM采用Fortran编程语言实现，经过了科学计算编程人员几十年的优化，性能已经极致，所以很难再去进一步的优化，在Nvidia的论文&lt;a href=&quot;http://arxiv.org/pdf/1410.0759.pdf&quot;&gt;cuDNN: Efficient Primitives for Deep Learning&lt;/a&gt;中指出了还存在着其他的一些方法，但是他们最后采用的还是改进的GEMM版本实现。GEMM可匹敌的对手是傅里叶变换，将卷积转为频域的相乘，但是由于在图像的卷积中存在strides，使得傅里叶变换方式很难保持高效。&lt;/p&gt;
</description>
        <pubDate>Tue, 29 Aug 2017 00:00:00 +0800</pubDate>
        <link>https://www.aiforall.pro//blog/2017/08/neural-network-layers-understanding/</link>
        <guid isPermaLink="true">https://www.aiforall.pro//blog/2017/08/neural-network-layers-understanding/</guid>
      </item>
    
      <item>
        <title>知行手记：毕业一周年</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;陷于思，简于情&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;六月的校园充满了告别与不舍，不管愿意与否，时间的巨浪一年又一年的将那些停靠在港湾的船儿推向未知的大海。这一切对于时间而言，不过是社会周期性的轮转，但对于每一条驶入大海的船而言，与大地相隔离开的那一刻，却是个体生命进程中最具仪式感的时刻之一——从此，风吹雨打日晒，或行进，或随流。&lt;/p&gt;

&lt;p&gt;如果说生活需要一些仪式感来刺激日复一日的平凡乃至碌碌无为，那么对于行将走完的六月之末，最大的仪式感便是自己已告别学生时代一周年了。所谓记录也是留念，博文余下部分是小白菜对自己半年来（去年下半年的状态&lt;a href=&quot;http://yongyuan.name/blog/year-turned-back.html&quot;&gt;2016年，归零清空&lt;/a&gt;）的&lt;strong&gt;思想&lt;/strong&gt;、&lt;strong&gt;情感&lt;/strong&gt;的一个总结，希望自己力行笃志，勿忘初心。&lt;/p&gt;

&lt;h2 id=&quot;陷于思&quot;&gt;陷于思&lt;/h2&gt;

&lt;p&gt;对于一个内心极其不安分的人而言，日复一复时间的轮转和实体空间中的条条框框都是一种桎梏。对此，小白菜深有同感并以此为然。小白菜深知自己不是一个很安分的人，虽然在现实生活中循规蹈矩，可是主导自我的精神体系却像一匹狂奔在草原的野马，总想着&lt;strong&gt;去浪荡、去经历、去过一种行无定处的生活&lt;/strong&gt;。这样一种精神体系的主导后果就是间歇性情绪低落症，如果在某一段较长时间内，在生活找不到有所期待的事物或者工作中无小有成就的喜感，便会陷入连日的情绪低落症，以至于在心里盘算着要不要make some changes。关于情绪低落甚而陷入沮丧，&lt;a href=&quot;https://www.douban.com/people/81194074/&quot;&gt;ErbB4不麋鹿&lt;/a&gt;说:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;我发现一个人生活时避免沮丧的方法就是始终有期待，比如很期待程序算出来的结果，或者很期待尝试的新菜谱的味道，或者是某天的活动，正在追的剧，正在看的书的情节发展，和盘算已久的旅行等等。如果没有期待，就会怀疑自己生活的意义，虽然思考意义没什么不好，但没有积极情绪的反弹，是很难走出来的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;想必间歇性情绪低落症也不是小白菜独自面临的问题，上面&lt;a href=&quot;https://www.douban.com/people/81194074/&quot;&gt;ErbB4不麋鹿&lt;/a&gt;所说的有所期待，说到底便是一个被讨论了无数次的hope问题。关于hope，小白菜觉得最好的解释，莫过于《肖申克的救赎》中Andy Dufresne所说的：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hope is a good thing, maybe the best of things and no good thing ever dies.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在这一点上，史铁生的&lt;a href=&quot;https://book.douban.com/subject/1136988/&quot;&gt;命若琴弦&lt;/a&gt;也有很深刻的探讨。作为一个上班族，在陷入日复一复上班的轮子里后，也难免在某一段时间内走着走着会陷入一定的迷茫，虽然工作上也有很明确的目标，有很多很多的问题要解，可是如果将时间拉得更长一些，比如两年、三年、五年后自己想实现什么样的一个大目标或者想成为什么样的一个人，便也只好用什么努力实现财务自由或者成为特定领域的专家来搪瓷。这样贴近“地气”的目标虽然是好，可是纯碎地靠这种方式去驱动，却也总是陷入自驱力的泥潭，精神状态一直无法摆脱周期性余弦振动的困扰。这一困扰的根本，小白菜以为在于信仰的缺失，只是小白菜却从未曾找到过，以至于小白菜的微信花名一直是“小白菜在寻找”。&lt;/p&gt;

&lt;p&gt;关于信仰，大部分人包括小白菜在内，都很难说是有信仰的。我们可能有一个、两个或是更多的理想，然而这些零星般的理想，却不过是我们生命中的或大或小的一部分，它可能在较长的一段时间内对我们很重要，但是在下一个循环结中，它对于当前的自己而言已经无关痛痒了。信仰却不同，信仰是一个人用尽自己全部的生命进程去追求、捍卫的一项崇高的事业或者精神准则，以至于到了&lt;strong&gt;无法坚持信仰的那一刻，他会选择宝贵生命来表达他最后的忠诚&lt;/strong&gt;，譬如&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E9%87%8B%E5%BB%A3%E5%BE%B7&quot;&gt;Thich Quang Duc&lt;/a&gt;为了捍卫宗教的信仰而选择火的殉道。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/imgs/posts/thich_quang_duc.jpg&quot; alt=&quot;Thich Quang Duc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一个有信仰的人是幸运地，无论在坚持自我、坚持信仰的道路上他都会获得极大的精神满足，就像&lt;a href=&quot;https://book.douban.com/subject/1858513/&quot;&gt;月亮与六便士&lt;/a&gt;中的思特里克兰德一样，他必须画画，就像溺水的人必须挣扎。关于信仰的种种探讨以及如何找到自己的信仰，小白菜自感功力浅薄，所谓历事勤读，答案也许潜藏于生活中，也有可能存在于某本书中，只是需要借由时间去参透。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;无善无恶心之体，有善有恶意之动，知善知恶是良知，为善去恶是格物&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;小白菜愿用一生的时间多读、多思，去理解、践行阳明心学，构建、稳固并完善自己的精神体系。&lt;/p&gt;

&lt;h2 id=&quot;简于情&quot;&gt;简于情&lt;/h2&gt;

&lt;p&gt;如果要用三句简短的话来概括友情、爱情和亲情这半年来的状态，小白菜以为最好的总结莫过如此：&lt;strong&gt;友情逐步沉淀，爱情迟迟未来，亲情依然照旧&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;友情&quot;&gt;友情&lt;/h3&gt;

&lt;p&gt;对于友情，小白菜也越来越赞同并接纳这样一种观点：闲来无事勿相扰。本科四年，读研三年，再到如今工作一年，从曾经的闹哄哄(各种八卦群)到现在各自回归自我平静的生活，与其说是情感的趋于平淡，倒不如说是各自生活和个人精神的沉淀。&lt;strong&gt;那些曾经美好的相处渐渐沉于记忆，而过于平淡的终将忘却，最后内心惦记并念念不忘的可能也就三两知心友人，或志趣相投、或曾经一块儿奋斗&lt;/strong&gt;。他们从不曾随意打扰过小白菜的生活，亦很少有这样的机会让小白菜去帮助过什么(问题他们总能自己搞定)。&lt;/p&gt;

&lt;p&gt;天各一方，岁月相伴，我们探讨技术，分享工作和生活的心得，心心相惜彼此的才能而乐于在合适的机会面前推举对方，小白菜以为，这就是最好最成熟的友情。&lt;/p&gt;

&lt;h3 id=&quot;爱情&quot;&gt;爱情&lt;/h3&gt;

&lt;p&gt;在绝大多数人都会步入的这条路上，爱情对于小白菜而言，还处于故事的起点。虽不愿随随便便找个人共度来日时光，却再也不肯多在上面做些功课。小白菜匠心情怀、锲而不舍、自信满满，但在爱情这条路上，在经历了一次失意后被打趴得止步不前。此后，在爱情这条路上，小白菜是如此的敏感，以致于蜷缩得像一只浑身是刺心也逐渐石化的刺猬，&lt;strong&gt;不轻易靠近人，也不想被无关的人接近&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;所以，从大学到如今工作满一年，时至8年，在历经了一场长达4年之久的单相思后，未曾再有过心动的妹子。也许，小白菜已经失去了喜欢一个人的能力，再或者，小白菜压根就不知道如何去爱一个人。&lt;/p&gt;

&lt;p&gt;过度的执着于爱情的相处理念，未必是一件好事。也只有到了“这把年纪”，才深切体会到爱情成为一种欠债和任务的无奈（苦笑）。蔡永康说：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;15岁觉得游泳难，放弃游泳，到18岁遇到一个你喜欢的人约你去游泳，你只好说“我不会耶”。18岁觉得英文难，放弃英文，28岁出现一个很棒但要会英文的工作，你只好说“我不会耶”。人生前期越嫌麻烦，越懒得学，后来就越可能错过让你动心的人和事，错过新风景。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;再说小白菜对于爱情的理解，在小白菜还小的时候，曾看到过这样一句话：真正的爱，不再于轰轰烈烈，也不再于信誓旦旦，而是在情感的全心投入中，加以责任的相伴，完成平静的相守。十几年后的今天，当小白菜一字一句敲出这段话来时，对于它的理解增加了几许。如果要用影像来表达爱情的释义，小白菜以为&lt;a href=&quot;https://movie.douban.com/subject/2129039/&quot;&gt;飞屋环游记&lt;/a&gt;和&lt;a href=&quot;https://movie.douban.com/subject/5327189/&quot;&gt;伦敦一家人&lt;/a&gt;足以。&lt;/p&gt;

&lt;h3 id=&quot;亲情&quot;&gt;亲情&lt;/h3&gt;

&lt;p&gt;特别喜欢归有光先生的&lt;a href=&quot;https://www.douban.com/group/topic/3096552/&quot;&gt;项脊轩志&lt;/a&gt;，一个人的小阁子，自言自语，哀而不伤，时至如今，仍能诵读一二：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;借书满架，偃仰啸歌，冥然兀坐，万簌有声；而庭阶寂寂，小鸟时来啄食，人至不去。三五之夜，明月半墙，桂影斑驳，风移影动，珊珊可爱。&lt;br /&gt;
······&lt;br /&gt;
庭有枇杷树，吾妻死之年所手植也，今已亭亭如盖矣。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;“庭阶寂寂，小鸟时来啄食，人至不去。三五之夜，明月半墙，桂影斑驳，风移影动，珊珊可爱”，像流水的生活、似脉脉的亲情，在喧闹的五道口，仍能在内心最柔弱的找到一处沉寂的&lt;a href=&quot;https://book.douban.com/subject/1865089/&quot;&gt;瓦尔登湖&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;小白菜特别希望老妈和老爸对生活充满了那么一点点的情调，譬如养一盆绿萝，在收到老姐送给的康乃馨时不是再磕磕叨叨又说乱花钱了······这一切希冀的小期待对于那些经历过大锅饭的父辈们来说，实在是苛求。&lt;/p&gt;
</description>
        <pubDate>Wed, 02 Aug 2017 00:00:00 +0800</pubDate>
        <link>https://www.aiforall.pro//blog/2017/08/graduate-after-one-year/</link>
        <guid isPermaLink="true">https://www.aiforall.pro//blog/2017/08/graduate-after-one-year/</guid>
      </item>
    
  </channel>
</rss>
